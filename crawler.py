import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import json


def crawl_saramin(keyword, pages=1):
    """
    ì‚¬ëŒì¸ ì±„ìš©ê³µê³ ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ ë° ê¸°ì—…ì •ë³´ ì¶”ê°€

    Args:
        keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        pages (int): í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

    Returns:
        DataFrame: ì±„ìš©ê³µê³  ì •ë³´ ë° ê¸°ì—…ì •ë³´ê°€ ë‹´ê¸´ ë°ì´í„°í”„ë ˆì„
    """

    jobs = []
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    ]

    for page in range(1, pages + 1):
        headers = {'User-Agent': random.choice(user_agents)}
        url = f"https://www.saramin.co.kr/zf_user/search/recruit?searchType=search&searchword={keyword}&recruitPage={page}"

        try:
            print(f"ğŸ” [{page}]í˜ì´ì§€ ìš”ì²­ ì¤‘: {url}")
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            print(f"âœ… [{page}]í˜ì´ì§€ ìš”ì²­ ì„±ê³µ (ìƒíƒœ ì½”ë“œ: {response.status_code})")

            soup = BeautifulSoup(response.text, 'html.parser')
            job_listings = soup.select('.item_recruit')
            print(f"ğŸ“‹ [{page}]í˜ì´ì§€ ì±„ìš©ê³µê³  ìˆ˜: {len(job_listings)}")

            for idx, job in enumerate(job_listings, start=1):
                try:
                    print(f"\n--- [{page}]í˜ì´ì§€ {idx}ë²ˆì§¸ ê³µê³  ---")

                    # íšŒì‚¬ëª…
                    company = job.select_one('.corp_name a').text.strip()
                    print(f"íšŒì‚¬ëª…: {company}")

                    # ì±„ìš© ì œëª©
                    title = job.select_one('.job_tit a').text.strip()
                    print(f"ì±„ìš© ì œëª©: {title}")

                    # ì±„ìš© ë§í¬
                    link = 'https://www.saramin.co.kr' + job.select_one('.job_tit a')['href']
                    print(f"ì±„ìš© ë§í¬: {link}")

                    # ê¸°ì—… ìƒì„¸ í˜ì´ì§€ ë§í¬
                    company_link = job.select_one('.corp_name a')['href']
                    company_url = f"https://www.saramin.co.kr{company_link}"
                    print(f"ê¸°ì—… ìƒì„¸ URL: {company_url}")

                    # ì§€ì—­, ê²½ë ¥, í•™ë ¥, ê³ ìš©í˜•íƒœ
                    conditions = job.select('.job_condition span')
                    location = conditions[0].text.strip() if len(conditions) > 0 else ''
                    experience = conditions[1].text.strip() if len(conditions) > 1 else ''
                    education = conditions[2].text.strip() if len(conditions) > 2 else ''
                    employment_type = conditions[3].text.strip() if len(conditions) > 3 else ''
                    print(f"ì¡°ê±´: ì§€ì—­={location}, ê²½ë ¥={experience}, í•™ë ¥={education}, ê³ ìš©í˜•íƒœ={employment_type}")

                    # íƒœê·¸ ê°€ì ¸ì˜¤ê¸°
                    badge = job.select_one('.area_badge .badge')
                    job_badge = badge.text.strip() if badge else ''
                    print(f"íƒœê·¸: {job_badge}")

                    # ë§ˆê°ì¼
                    deadline = job.select_one('.job_date .date')
                    deadline_text = deadline.text.strip() if deadline else 'ë§ˆê°ì¼ ë¯¸ê¸°ì¬'
                    print(f"ë§ˆê°ì¼: {deadline_text}")

                    # ë“±ë¡ì¼ (job_day í´ë˜ìŠ¤)
                    registration = job.select_one('.job_day')
                    registration_text = registration.text.strip().replace("ë“±ë¡ì¼ ", "") if registration else 'ë“±ë¡ì¼ ë¯¸ê¸°ì¬'
                    print(f"ë“±ë¡ì¼: {registration_text}")

                    # ì§ë¬´ ë¶„ì•¼
                    job_sector = job.select_one('.job_sector')
                    sector = job_sector.text.strip() if job_sector else 'ì§ë¬´ë¶„ì•¼ ë¯¸ê¸°ì¬'
                    print(f"ì§ë¬´ë¶„ì•¼: {sector}")

                    # ê¸°ì—… ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    company_info = get_company_details(company_url)
                    print(f"ê¸°ì—… ìƒì„¸ ì •ë³´: {company_info}")

                    jobs.append({
                        'íšŒì‚¬ëª…': company,
                        'ì œëª©': title,
                        'ë§í¬': link,
                        'ì§€ì—­': location,
                        'ê²½ë ¥': experience,
                        'í•™ë ¥': education,
                        'ê³ ìš©í˜•íƒœ': employment_type,
                        'íƒœê·¸': job_badge,
                        'ë§ˆê°ì¼': deadline_text,      # ë§ˆê°ì¼
                        'ë“±ë¡ì¼': registration_text,  # ë“±ë¡ì¼
                        'ì§ë¬´ë¶„ì•¼': sector,            # ì§ë¬´ë¶„ì•¼
                        'ì—…ì¢…': company_info.get('ì—…ì¢…', 'N/A'),
                        'í™ˆí˜ì´ì§€': company_info.get('í™ˆí˜ì´ì§€', 'N/A'),
                        'ì£¼ì†Œ': company_info.get('ì£¼ì†Œ', 'N/A'),
                        'ëŒ€í‘œìëª…': company_info.get('ëŒ€í‘œìëª…', 'N/A'),
                        'ì‚¬ì—…ë‚´ìš©': company_info.get('ì‚¬ì—…ë‚´ìš©', 'N/A')
                    })

                except AttributeError as e:
                    print(f"âš ï¸ í•­ëª© íŒŒì‹± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
                    continue

            print(f"\nâœ… [{page}]í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ")
            time.sleep(random.uniform(2, 4))  # í˜ì´ì§€ ê°„ ëœë¤ ë”œë ˆì´ ì¶”ê°€

        except requests.RequestException as e:
            print(f"âŒ í˜ì´ì§€ ìš”ì²­ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            continue

    return pd.DataFrame(jobs)


def get_company_details(company_url):
    """
    ê¸°ì—… ìƒì„¸ í˜ì´ì§€ì—ì„œ 'ì—…ì¢…', 'í™ˆí˜ì´ì§€', 'ì£¼ì†Œ', 'ëŒ€í‘œìëª…', 'ì‚¬ì—…ë‚´ìš©' ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    company_info = {
        'ì—…ì¢…': 'N/A',
        'í™ˆí˜ì´ì§€': 'N/A',
        'ì£¼ì†Œ': 'N/A',
        'ëŒ€í‘œìëª…': 'N/A',
        'ì‚¬ì—…ë‚´ìš©': 'N/A'
    }

    # ëœë¤ User-Agent ì„¤ì •
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    ]
    headers = {'User-Agent': random.choice(user_agents)}

    try:
        print(f"ğŸ” ê¸°ì—… ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì¤‘: {company_url}")
        time.sleep(random.uniform(1, 3))  # ëœë¤ ë”œë ˆì´ ì¶”ê°€
        response = requests.get(company_url, headers=headers)
        response.raise_for_status()
        print(f"âœ… ê¸°ì—… ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì„±ê³µ")

        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')

        # ê¸°ì—… ì„¸ë¶€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        details_groups = soup.select('.company_details_group')
        for group in details_groups:
            title = group.select_one('.tit')
            desc = group.select_one('.desc')
            if title and desc:
                title_text = title.text.strip()
                desc_text = desc.text.strip()

                if title_text == 'ì—…ì¢…':
                    company_info['ì—…ì¢…'] = desc_text
                elif title_text == 'í™ˆí˜ì´ì§€':
                    company_info['í™ˆí˜ì´ì§€'] = desc_text
                elif title_text == 'ì£¼ì†Œ':
                    company_info['ì£¼ì†Œ'] = desc_text
                elif title_text == 'ëŒ€í‘œìëª…':
                    company_info['ëŒ€í‘œìëª…'] = desc_text
                elif title_text == 'ì‚¬ì—…ë‚´ìš©':
                    company_info['ì‚¬ì—…ë‚´ìš©'] = desc_text

    except Exception as e:
        print(f"âš ï¸ ê¸°ì—… ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

    return company_info


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    df = crawl_saramin('javascript', pages=3)
    print(df)
    df.to_json('crawled_data.json', orient='records', force_ascii=False, indent=4)
